{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Apprentissage, M2CHPS 2023\n",
    "\n",
    "## Assignments - Part 2\n",
    "\n",
    "**Instructor:** [Argyris Kalogeratos](http://kalogeratos.com) (contact: argyris.kalogeratos@ens-paris-saclay.fr).  \n",
    "**Sylabus:** [http://nvayatis.perso.math.cnrs.fr/CHPScourse.html](http://nvayatis.perso.math.cnrs.fr/CHPScourse.html)\n",
    "\n",
    "The most updated version of this file can be found at instructor's webpage:  \n",
    "[http://kalogeratos.com/MyCourses/DML-M2HPC/Assignments2.ipynb](http://kalogeratos.com/psite/files/MyCourses/DML-M2HPC/Assignments1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Before you start\n",
    "\n",
    "* This assignement has already few exercises and  will be completed 2 weeks before the final deadline. You can start working on them and get any update as you go.\n",
    "* Work on the assignment either alone or in a team of two (pair). Students that will work alone will benefit from a 15% bonus. Important: The point in working in a pair is not to split the work, but to collaborate! \n",
    "* Prepare **a single jupiter notebook** in a report-like format for all the exercises. Use this notebook as a starting point and place your answers just below each exercise. Describe properly what you do in each step of your solution, followed by your well-commented code implementation that can automatically produce figures and numerical results. Results should also be well-commented and discussed. Your comments can be either in English or in French.\n",
    "* If you need to provide mathematical expressions, incorporate all that in your notebook in a latex form (not handwritten!). \n",
    "* Important: send your notebook that includes all your work and any additional files you used (e.g. datasets) at by email, with a title ``M2CHPS <year> - <Surname1> & <Surname2>``. The title of the notebook should have the surnames of the students. The body of the email should also mention the students' names and how they split the work; if you face difficulties in clarifying this with your classmate, please write separate emails with your views. \n",
    "* Always use references for things you reuse (e.g. internet, forums, books, published papers, etc.)\n",
    "* Do not copy-paste from the internet recipies that do the asked thing (or what is most usually observed: to use something relevant but not exactly what is asked). You need to understand what you use (sources should be properly referenced), adapt it to what each exercise asks for, and comment clearly the results. Poorly commented and explained pieces of work/code will not be taken seriously into account. \n",
    "* Copying from one another is not acceptable. We are strict with that: cheating in even one exercise will incur **a zero to all projects** (not just one assignement) and possible disqualification from the exams. Your work will be cross-checked against anything similar submitted from past students of the course.\n",
    "* The assignments will be examined orally at the end of the semester. Each group of students should be able to explain in about 10 mins all what they did, and answer short questions.\n",
    "* **Deadline: midnight of Sunday 1/12.**\n",
    "* **Late submissions:** the maximum possible grade is penalized by 5%/day. E.g. when delivering on time the best grade is 20/20, late submission by one day gets at best 19/20, for two days late of delay 18/20...  \n",
    "* **Slot for questions: Friday 15/11 at 14h**. Zoom link: https://kalogeratos.com/zoom. Meeting ID: 243 940 6970 -- Passcode: 15881598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 [15%]\n",
    "\n",
    "A security system relies on a battery ($b$), two sensors ($c$ and $d$), and an alarm ($a$). The random variables $a$, $b$, $c$ are binary, and $a=1$ when the alarm fires, $b=1$ when the battery functions properly, and $c=1$ and $d=1$ when the respective sensor functions properly. Otherwise, each variable takes the value $0$.\n",
    "\n",
    "The architecture of the system is as follows: the battery provides power with independent cables to the two sensors, and the sensors feed information to the alarm that can then fire to signify a threat. Each sensor can operate even with only its internal power source that it is equipped with, but in that case it becomes much more sensitive to fail compared to when the external battery is functioning normally. \n",
    "\n",
    "The manual of the system says that the probability for the battery to fail is 0.1, and that each sensor fails with probability 0.01 when the battery is ok, and with 0.7 when the battery is not ok. \n",
    "\n",
    "Additionally, the manual also informs us that if at least one sensor works fine, then the alarm fires with probability 0.02. Otherwise the alarm fires with 30 times higher probability.\n",
    "\n",
    "**a)** Draw the graphical model of the system involving the random variables.\n",
    "\n",
    "**b)** Compute the joint probabilities $p(a,b,c,d)$ and $p(a,c)$. Also check if $c$ and $d$ are independent.\n",
    "\n",
    "**c)** Suppose **the alarm is silent** and that we check that **only the sensor $d$ works properly**. What is the posterior of having a disfunctioning battery?\n",
    "\n",
    "**d)** Compute the probability for the alarm to be silent, $p(a=0)$.\n",
    "\n",
    "**e)** We know that when at least one sensor works fine then the alarm fires with 0.02 (which let's suppose it is as much as as it is normal), otherwise with 0.6. We want to know how many sensors should the system have (connected like $c$ and $d$ in the initial design) such that the total probability $p(a=1) < 0.05$.\n",
    "\n",
    "**f)** Suppose **we don't hear any alarm** (with good ears!) and **we have no information about the sensors** functioning. What is the probability for the battery to be disfunctioning?\n",
    "\n",
    "**g)** Implement a function that generates observations from the probabilistic model at the top. Then make simulations to confirm empirically the estimations you calculated by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - [30%]\n",
    "\n",
    "**Isolation Forests (IF)** are efficient in identifying global outliers. IF can provide with an outlier index for each datapoint included in a given dataset; let that be denoted by **IF_index**. \n",
    "\n",
    "On the other hand, [**Local Outlier Factor (LOF)**](https://en.wikipedia.org/wiki/Local_outlier_factor) identifies efficiently local outliers, although it's computationally demanding for large datasets, and is theoretically weaker as dimensions grow larger due to relying on density-based calculations (recall the *curse of dimensionality*). Before giving the questions of the exercise, let's see the details of computing LOF (see the updated Lab3 file for technical explanations about how to compute LOF in practice).\n",
    "\n",
    "For this exercise you are asked the following:\n",
    "\n",
    "**a)** Implement your own version of LOF and demonstrate a comparison with the built-in version of scikit-learn using the example we saw in the lab. Plot in a meaningful way the differences between the LOF index you compute for each point and what the built-in version gives. (note: since there are different LOF variants, there is the possibility that your correct implementation may give slightly different index values, but this is ok).\n",
    "\n",
    "**b)** Investigate and propose a way to combine LOF with IF. More specifically, a way to introduce to the computations of the LOF method the IF_index we mentioned at the top of the description. \n",
    "\n",
    "**c)** Provide a meaningful demonstration with several examples that compare LOF, IF, and the hybrid LOF+IF.\n",
    "\n",
    "**References:**\n",
    "M.M. Breunig et al. [LOF: Identifying Density-Based Local Outliers](https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf), Proc. ACM SIGMOD Int. Conf. On Management of Data, 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - [45%]\n",
    "\n",
    "As opposed to bottom-up clustering approaches, a top-down method starts with one cluster -a model with one component- and tries incrementally to increase the number of clusters by introducing and fitting more components in the model. The general algorithm goes as follows:\n",
    "\n",
    "* **Initialization:** start with $k$=1 component model, $M_k$\n",
    "* **Step 1:** fit the model $M_k$\n",
    "* **Step 2:** find a good way to introduce and initialize a new component; set $k=k+1$ and go to Step 1 \n",
    "\n",
    "There are many ways to make more specific the steps of this process. \n",
    "\n",
    "**Model:** one can pick a cluster model, e.g. a centroid-based like that of k-means, or a probabilistic model like the Gaussian Mixture Models (GMM). In both cases an iterative approach takes place when fitting such a model.\n",
    "\n",
    "**Incremental step:** Most importantly, one needs to decide about how Step 2 operates. Indicative options originally expressed using $k$-means are:\n",
    "* **Bisecting:** This approach chooses one cluster and splits it in two parts. The criterion of which cluster to pick varies; one simple way is to choose the cluster with the largest error (or average error). In the case of k-means, which uses a hard assignment of points to clusters, this is: \n",
    "    - B1) Initialize a second centroid using a random point of the cluster.\n",
    "    - B2) Run a 2-means inside the cluster using only the points of that cluster. \n",
    "    - B3) (Optionally) A full k-means fine-tuning to update all the $k+1$ clusters can be performed. \n",
    "    Note that, if step B3 is not involved, 2 of the centroids are the product of the cluster splitting, while the rest come untouched from the previous $k$-order solution.\n",
    "* **Global:** In its greedy version (see if needed the reference), this approach examines as the ($k+1$)-th centroid every of the $N$ points of the dataset, and in each case it fits a different $k+1$ model with ($k+1$)-means. Out of all the $N$ trials, it chooses the model that produced the maximal reduction of the clustering error (in fact it doesn't need to store the models, but only to rerun the best case found).\n",
    "* **Fast Global:** It is a variation of Global k-means that initializes the $k+1$-th centroid with the point that achieves the largest reduction of error only after one k-means update, i.e. after considering it as new centroid and looking which other points would be assigned to that new cluster, and without iterating further till convergence.\n",
    "* **Global++:** Instead of checking all $N$ points as candidates for initializing the $k+1$-th centroid, here the k-means++ principle is used. Specifically, the $k+1$ centroid is initialized by choosing at random one point based on the distribution of the inverse sum of distances of each point to the $k$ computed centroids. This should be repeated $m<N$ times (where $m$ is a parameter, e.g. $m=\\lfloor 0.1*N \\rfloor$) and pick the best choice.\n",
    "\n",
    "**Recipe to prepare**\n",
    "\n",
    "Consider that the final number of clusters $k^*$ is known in each case. First, use $k$-means as base algorithm, then extend to GMM as described below.\n",
    "\n",
    "**a)** Implement the greedy global incremental, the fast global, and the global++ variations.\n",
    "\n",
    "**b)** Which property of the solution produced by the bisecting variation does not hold any more, when including Step B3 and after applying it? Compare with the global incremental approach with regards to the same property?\n",
    "\n",
    "**c)** Run experiments for the color clustering application we saw in the course (with given $k^*$ as in that example).\n",
    "\n",
    "**d)** Construct a random synthetic dataset as follows. Consider a 5-by-5 2D grid arrangement of 25 clusters: i.e. the mean of the 1st cluster will be at (0,5), the 2nd at (1,5),..., the 6th at (0,4),..., the 25th at (5,0)). Each cluster will have $N=50$ points in 2D generated by a Gaussian distribution, with fixed $\\sigma$ for all clusters (test values $\\sigma = ${0.1, 0.5, 0.7, 1}). Run experiments with all your implementations (i.e. flat GMM training with fixed $k$; incremental GMM training with bisecting splits; incremental GMM training based on the global principle). Provide summaries of results about the clustering error and detailed comments with your observations/interpretations about the performance of the methods.\n",
    "\n",
    "**e)** Now, extend your toolbox by also considering the GMM as base algorithm. You will need a piece of code implementing the GMM training with a fixed $k$ using the Expectation-Maximization (EM) algorithm. \n",
    "\n",
    "**f)** What is the main problem of applying the bisecting principle in the GMM case? Find a heuristic workaround to adapt it to this probabilistic cluster model.\n",
    "\n",
    "**References:** \n",
    "- A. Likas et al., [The global k-means algorithm](https://hal.inria.fr/inria-00321515/document). Pattern Recognition, 2003.\n",
    "- G. Vardakas et al., [Global k-means++: an effective relaxation of the global k-means clustering algorithm](https://arxiv.org/pdf/2211.12271), 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - [10%]\n",
    "\n",
    "Given a dataset $X = \\{x_1,...,x_N\\}$, $x_i \\in \\mathbb{R}^d$ and a distance measure computed by the function $dist(x_i, x_j) : X \\times X \\mapsto \\mathbb{R}$, one can define a procedure that *reweights* the distances as: \n",
    "$$\n",
    "    dist'(x_i, x_j) = w_{ij} * dist(x_i, x_j)\n",
    "$$\n",
    "where $w_{ij} \\in \\mathbb{R}^+$. The aim is to bring datapoints that are related closer and to distanciate others that are less related. \n",
    "\n",
    "* **Kernels:** A typical low-level approach used is by employing a kernel, e.g. the RBF-kernel (radial basis function) \n",
    "    $$w_{ij} := 1 / (1+RBFkernel(x_i, x_j)) = 1 / (1 + \\exp\\{-\\gamma \\Vert x_i - x_j \\Vert^2_2\\})$$\n",
    "where $\\gamma = \\frac{1}{2\\sigma^2}$. \n",
    "\n",
    "* **Cluster-based:** Alternatively, cluster-based high-level approaches can be employed. The idea is that we could cluster the dataset and exploit its cluster structure to determine a better $w_{ij}$. \n",
    "    * Flat clustering (e.g. $k$-means): Generate one or more clustering results and compute the frequency the two datapoints $x_i$ and $x_j$ get clustered together. \n",
    "    * Hierarchical clustering (e.g. HAC or bisecting $k$-means): Given the learned cluster hierarchy, we could define the proximity between two datapoints $x_i$ and $x_j$ based on the depth in the hierarchy (let that be $h_{ij}$) at which they get separated. \n",
    "    * Incremental clustering (e.g. global $k$-means): (deliberately not explained how this would work).\n",
    "    \n",
    "For this exercise, you will need to: \n",
    "\n",
    "a) Implement the above concepts and approaches: the simple kernel-based approach, and the three cluster-based approaches. For the incremental clustering one, you will need to devise a way to build a meaningful measure.\n",
    "\n",
    "b) Use generated synthetic data, like those in the previous exercise, to demonstrate the correctness of your implementation and the behavior of the approaches. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
