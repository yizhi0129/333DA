{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Apprentissage, M2CHPS\n",
    "\n",
    "## Assignments - Part 1\n",
    "\n",
    "**Lecturer:** [Argyris Kalogeratos](http://kalogeratos.com) (contact: argyris.kalogeratos@ens-paris-saclay.fr).   \n",
    "**Sylabus:** [http://nvayatis.perso.math.cnrs.fr/CHPScourse-2024.html](http://nvayatis.perso.math.cnrs.fr/CHPScourse-2023.html)\n",
    "\n",
    "The most updated version of this file can be found at the webpage:  \n",
    "[http://kalogeratos.com/MyCourses/DML-M2HPC/Assignments1.ipynb](http://kalogeratos.com/psite/files/MyCourses/DML-M2HPC/Assignments1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Before you start\n",
    "\n",
    "* Work on the assignment either alone or in a team of two (pair). Students that will work alone will benefit from a 15% bonus. Important: The point in working in a pair is not to split the work, but to collaborate! \n",
    "* Prepare **a single jupiter notebook** in a report-like format for all the exercises. Use this notebook as a starting point and place your answers just below each exercise. Describe properly what you do in each step of your solution, followed by your well-commented code implementation that can automatically produce figures and numerical results. Results should also be well-commented and discussed. Your comments can be either in English or in French.\n",
    "* If you need to provide mathematical expressions, incorporate all that in your notebook in a latex form (not handwritten!). \n",
    "* Important: send your notebook that includes all your work and any additional files you used (e.g. datasets) at by email, with a title ``M2CHPS <year> - <Surname1> & <Surname2>``. The title of the notebook should have the surnames of the students. The body of the email should also mention the students' names and how the work was split among them; if you face difficulties in precising this with your classmate, please write individual emails. \n",
    "* Always use references for things you reuse (e.g. internet, forums, books, published papers, etc.)\n",
    "* Do not copy-paste from the internet recipies that do the asked thing (or what is most usually observed: to use something relevant but not exactly what is asked). You need to understand what you use (sources should be properly referenced), adapt it to what each exercise asks for, and comment clearly the results. Poorly commented and explained pieces of work/code will not be taken seriously into account. \n",
    "* Copying from one another is not acceptable. We are strict with that: cheating in even one exercise will incur **a zero to all projects** (not just one assignement) and possible disqualification from the exams. Your work will be cross-checked against anything similar submitted from past students of the course.\n",
    "* The assignments will be examined orally at the end of the semester. Each group of students should be able to explain in about 10 mins all what they did, and answer short questions.\n",
    "* **Deadline: midnight of Monday 20/10**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - [30%]\n",
    "Read about the iterative [RANdom SAmple Consensus (RANSAC)](https://en.wikipedia.org/wiki/Random_sample_consensus) method for robust linear regression. Consult also the material of the Lab and the documentation of scikit learn.\n",
    "\n",
    "**a)** Implement from scratch your own code for the RANSAC algorithm. You can still use as a building block the native linear regression of scikit learn in your implementation. Then, compare it with the simple linear regression and the build-in RANSAC implementation of scikit learn.\n",
    "\n",
    "a1) Apply your solution to the dataset of the example given in [Lab 2](http://kalogeratos.com/MyCourses/DML-M2HPC/Lab2.ipynb) to see that it  matches the behavior of the already coded method of scikit learn. Create plots including the linear models found by the typical linear regressor, the scikit RANSAC, and you version(s).\n",
    "\n",
    "a2) Apply your method to the following cases:\n",
    "* Use only the part of the inliners that comes from the inclined linear model (about 1000 points). Add extra $p$% of noisy points (where $p$ can be 5%, 10%, 20%, 30%) by picking random elements of the data and perturbing them \"sufficiently\" with a Gaussian noise so that they get dislocated far from the rest of the data on both sides.\n",
    "* Do as before, but constrain the perturbation to be only positive (all noisy points will be above the inline data).\n",
    "* Do as before, but the perturbed data will not be randomly selected, but rather correspond to the $p$% of the points with lower $y$ coordinate (the points at the lower left part of the plot).\n",
    "\n",
    "What differences do you observe among these cases. Report with illustrative results and comments. You can test additional cases using the same data generation code, after changing some of the parameters (e.g. the number of outliers and their distance to the inliers).\n",
    "\n",
    "**b)** The original RANSAC uses inlying data points as votes for candidate models. An alternative is to use the opposite approach: i.e. to use the models to vote for the most inlying points. Implement this variation that operates as follows:\n",
    "- Each time you subsample from the data, build a linear model, all inliers get a vote +1.\n",
    "- After having built the predefined number of models, use the top q% (e.g. 75%) voted points and build the final linear model using only those points.\n",
    "\n",
    "Use the previous dataset or/and other variations you may generate to demonstrate differences in performance, and discuss the results.\n",
    "\n",
    "**c)** As discussed in the class, the logic behind the RANSAC could actually be seen as a more general tool than just performing regression. More specifically, it can be seen as a way to perfom various kinds of *robust estimation*. \n",
    "\n",
    "<img src=\"./cloud-of-points-outliers.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "c1) Assume for example the problem where we have a point-cloud of data and we want to estimate their \"center\", as a group representative, but in a robust way. The figure above illustrates an example where the input data contains $100$ inliers and $10$ outliers, which however you don't know a priori. Here, the inliers come from a Gaussian distribution with $\\mu = (0,0)$ and $\\sigma = 0.83$, while the ouliers come from a Gaussian dislocated at $\\mu = (0,0)$ and $\\sigma = 1.5$. You can get the [.mat data file](http://kalogeratos.com/psite/files/MyCourses/DML-M2HPC/cloud-of-points-data.mat) with this example and/or write your own code to generate it. \n",
    "\n",
    "How would you design a solution to this problem that adopts the RANSAC principle so that for this kind of data input we can compute a representative as close as possible to the mean of the inliers?\n",
    "\n",
    "c2) Extend this approach to the clustering of the data in a given number of $k$ clusters by considering a centroid-based clustering method (e.g. $k$-means) as the  internal model learner. Provide an implementation and demonstrate some example(s).\n",
    "\n",
    "c3) Write an abstract pseudocode that expresses the generalization of RANSAC in arbitrary statistical estimation or learning problems? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - [15%]\n",
    "\n",
    "In Section 2.5 of the book \"[The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print12_toc.pdf)\", by T. Hastie, R. Tibshirani, and J. Friedman, there is a mention about the effects of the *curse of dimensionality*. Read the section and implement the code to reproduce and verify all the experiments shown in the Figures 2.6, 2.7, 2.8, and 2.9.\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - [15%]\n",
    "\n",
    "**a)** The rule of thumb for **train/test split** ratio is to choose it to be quite larger than 1 (e.g a 75%-25% train/test split). Validate empirically this choice:\n",
    "\n",
    "* Choose a dataset for a classification task.\n",
    "* Use a classification method that is suitable for the problem (i.e. performs reasonably well on the dataset).\n",
    "* Try several train/test split ratio values (e.g. 0% to 100% with step 5%).\n",
    "* Use plot(s) to display the performance of the method on the test set in each case.\n",
    "* Justify in detail the observed behavior using arguments founded in the theory (see bias-variance trade off).\n",
    "* What is (approx.) the best choice of ratio in your case?\n",
    "* Now, using a train/validation/test split, optimize the hyperparameters of your model, and give the final trained model that you would deliver for practical use. \n",
    "\n",
    "**b)** *Cross-validation* splits the set of data in non-overlapping parts, then learns and tests the performance of a method independently in the different parts of the data, e.g. i) train-test set split, or ii) k-folded approach. \n",
    "\n",
    "Imagine that you have to apply (i) and (ii) for validating a regression method for time-series, a task which is also referred as *time-series forecasting*. Note that in this case the element of time should be respected.\n",
    "\n",
    "Describe a procedure, by giving a pseudocode, that would apply (i) and (ii).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - [20%]\n",
    "\n",
    "The purpose of this exercise is to apply classification for two data classes using a **simple linear model**. However, the particularity is that we are in the context of *stream processing* where data arrive sequentially in pairs of observations, containing one observation from each class, i.e. each time $t$ we observe $(x_t \\in \\text{class A}, x'_t \\in \\text{class B})$. As *we cannot store all the data* to apply batch processing, we need to develop a frugal online learning approach that would require as little as possible stored information at each time $t$ for a simple linear model $model_t$, which then would be updated based on the incoming $(x_t, x'_t)$'s. At any time $t$ the model $model_t$ should be able to classify uknown data.\n",
    "\n",
    "**a)** Analyze in detail the requirements of this problem setting, and implement an approach to produce a linear model for all the data of the stream. Explain why it is correct and how much memory resources it would need to run.\n",
    "\n",
    "**b)** To train your model consider $N = n+n$ data produced two 2D Gaussian distributions ($n$ instances from each of them) that have (e.g.) same standard deviation and different mean values $\\mu_1$ and $\\mu_2$. Use a large $N$ (e.g. $500$) and show the performance of $model_t$ at the course of time (i.e. for $t=1,...,N$) using a test-set of $500 + 500$ datapoints generated from the same distributions. Compare in the same plot (e.g. it would appear as fixed horizontal line) against a linear classifier that is trained directly on the $N$ datapoints. Use a couple of $\\mu_1$ and $\\mu_2$ pairs to show an easy case and a more difficult one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - [10%]\n",
    "\n",
    "Relying on the central hypothesis that a learning problem should enjoy regularity along the domain of the data (see [Lab 1](http://kalogeratos.com/MyCourses/DML-M2HPC/Lab1.ipynb)), we would like to define a heuristic data-driven measure to evaluate a given model.\n",
    "\n",
    "More specifically, consider a classification problem and define a regularity measure based on the nearest neighbors principle that can assess how regular the decisions of a given classifier is. Explain the logic of your proposition. Let's call this function **Regularity(X, Y, +any needed parameters)**.\n",
    "\n",
    "**Recipe for testing**\n",
    "\n",
    "In this exercise you have to generated data from a mixture of 2 Gaussian distributions, and then measure the effect of your Regularity measure and the standard Accuracy with regards to the following factors:\n",
    "\n",
    "* the size of the dataset; try generating classes of same standard deviation $\\sigma = 2$ and equal size, where each of them will have $n_i\\in\\{50, 100, 200, 500\\}$ objects;\n",
    "* the dimensionality $d$ of the data; try $d\\in\\{1,2,3,...\\,,10, 20, 30,...\\,100\\}$;\n",
    "* the overlap of the classes $\\delta$ in the feature space, i.e. the distance between the classes; try $\\delta\\in\\{\\sigma, 2\\sigma, 3\\sigma, 4\\sigma\\}$;\n",
    "* the input parameter $k$ which indirectly adjusts how far from an object the classifier will seek for `advices' from other objects in order to classify it; try $k\\in\\{1,...\\,,10\\}$.\n",
    "\n",
    "**Sample pseudocode:**  \n",
    "\n",
    "for each tuple <$n$, $d$, $\\delta$, $k$>\n",
    "   * generate data  \n",
    "   * compute average quality over $10$ runs of the following  \n",
    "       * train a $k$NN classifier and a decision tree (choose nicely the depth)   \n",
    "       * test the classifiers using Accuracy and Regularity  \n",
    "\n",
    "end for\n",
    "\n",
    "The evaluation should be performed with and without 10-fold CV (inside the train/test nested part). Use at least the suggested value range for each parameter, but you can extend it if this way you capture some interesting behavior. \n",
    "\n",
    "**Questions:** \n",
    "- What is the main difference between a $k$NN and a radius-NN classifier? In which case(s) one is better to the other? How would we have to adapt the above discussion if radius-NN classifier is used instead of the standard $k$NN?\n",
    "- What would change if we were interested in measuring the Regularity in the context of a regression problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 - [10%]\n",
    "\n",
    "Apply ridge regression and lasso regression on the diabetes dataset. Use all the features but the 11th as input parameters, and the 11th as target (it's an index showing the progression of the disease). \n",
    "* Search for a good $a$ regularization penalty using Cross Validation (use built-in functions of scikit-learn) for each mehtod, as in the examples 4 and 5 of Lab 2. \n",
    "* Generate plots that show the evolution of all the coefficients (the elements of the $w$ vector) and -in a second plot- regression scores such as the resiguals and the [$R^2$ measure](https://en.wikipedia.org/wiki/Coefficient_of_determination) showing how well the model fits to the data.\n",
    "* Discuss your comparative findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
